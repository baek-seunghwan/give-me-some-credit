# 🎤 면접 Q&A 가이드

**신용위험 예측 AI 모델 프로젝트 - 기술 면접 대비**

---

## 📋 목차
1. [프로젝트 관련 질문](#프로젝트-관련-질문)
2. [기술 심화 질문](#기술-심화-질문)
3. [문제 해결 능력 질문](#문제-해결-능력-질문)
4. [역량 확인 질문](#역량-확인-질문)

---

## 프로젝트 관련 질문

### Q1: 이 프로젝트를 하게 된 동기는 무엇인가요?

**답변 전략**: 배운 점, 흥미, 실무 연관성 강조

```
A: 머신러닝을 실제 비즈니스 문제에 어떻게 적용하는지 배우고 싶었습니다.
특히 금융 분야의 신용 평가는 실무적으로도 중요하고, 데이터 품질 처리부터
모델 평가까지 전체 파이프라인을 경험할 수 있는 프로젝트라고 생각했습니다.

Kaggle의 "Give Me Some Credit" 데이터셋을 선택한 이유는:
- 현실적인 불균형 데이터 처리 경험
- 다양한 알고리즘 비교 기회
- 실제 금융 시스템에서의 응용 가능성
```

**팔로우업 질문 대비**:
- "이 경험이 앞으로의 커리어에 어떤 영향을 줄 것 같나요?"
  → "데이터 기반 의사결정의 중요성을 배웠고, 실제 제품 개발 때 이런 경험이 유용할 것 같습니다"

---

### Q2: 프로젝트에서 가장 어려웠던 부분은?

**답변 전략**: 문제 → 해결 → 배운 점 순서로

```
A: 클래스 불균형 처리가 가장 어려웠습니다.

[문제]
- 부도(74%) vs 정상(26%) = 약 3:1 불균형
- 단순히 Accuracy만 보면 정상으로만 예측해도 74% 정확도
- 하지만 부도자를 놓치는 것이 비즈니스적으로 위험

[해결 과정]
1. 처음엔 Accuracy만 봤는데, F1-Score를 도입
2. 클래스 가중치(class_weight='balanced') 적용
3. 임계값 조정 시도
4. 최종적으로 Recall과 F1-Score를 중시한 평가 지표 선택

[배운 점]
- 메트릭 선택이 모델 성능 평가만큼 중요
- 비즈니스 문맥에 맞는 평가 기준이 필요
- 불균형 데이터를 다루는 다양한 기법 학습
```

---

### Q3: 데이터 전처리에서 가장 중요한 부분은 무엇이었나요?

**답변 전략**: 각 단계의 목적과 효과를 명확히

```
A: 이상치 제거(49%)와 정규화가 가장 중요했습니다.

[이상치 제거 - IQR 방법]
- 범위: Q1 - 1.5×IQR ~ Q3 + 1.5×IQR
- 효과: 극단적 값 제거로 모델 안정성 향상
- 49% 제거했지만 남은 데이터의 품질이 훨씬 좋음

[정규화 - Min-Max Scaling]
- 범위: [0, 1]
- 이유: 
  1) 스케일 불일치 해결 (나이: 21~96, 소득: 0~12,596)
  2) SVM 같은 거리 기반 알고리즘 성능 향상
  3) 경사하강법 수렴성 개선

[검증]
전처리 전후 모델 성능 비교로 효과 입증
```

---

## 기술 심화 질문

### Q4: IQR 방식을 선택한 이유는? 다른 방법과의 비교는?

**답변 전략**: 장단점 비교, 데이터 특성 고려

```
A: 세 가지 방법을 고려했습니다.

[1] Z-score 방식
   이상치: |z| > 3
   - 장점: 간단함
   - 단점: 정규분포 가정, 극단값에 민감
   
[2] IQR 방식 (선택함)
   이상치: Q1-1.5×IQR 또는 Q3+1.5×IQR
   - 장점: 극단값에 덜 민감, 분포 가정 불필요
   - 단점: 덜 엄격할 수 있음
   
[3] Isolation Forest
   - 장점: 복잡한 패턴 탐지
   - 단점: 계산 복잡도 높음, 해석 어려움

[선택 이유]
실제 금융 데이터는 정규분포를 따르지 않으므로 Z-score는 부적절.
IQR은 실무에서 가장 널리 쓰이고, 강건성이 뛰어남.
Isolation Forest는 해석이 어려워 금융 규제 이슈 가능.
```

**팔로우업**: "이상치 제거 49%가 너무 많지 않나요?"
```
→ 네, 맞습니다. 실제로:
  1) 훈련 데이터: 105,000 → 53,362 (51%)
  2) 테스트 데이터: 45,000 → 22,805 (51%)
  3) 일관된 비율로 유지되었으므로 체계적임

  만약 한쪽만 많이 제거됐다면 데이터 누수 의심
  하지만 양쪽이 같은 비율이므로 유효한 이상치 제거
```

---

### Q5: Min-Max 정규화 vs 표준화(Standardization), 어느 것이 맞나요?

**답변 전략**: 상황별 선택 기준 설명

```
A: 상황에 따라 다릅니다.

[Min-Max Scaling]
공식: x' = (x - min) / (max - min)
범위: [0, 1]
사용: 범위를 알 때, 신경망, SVM
- 장점: 범위가 명확, 해석 쉬움
- 단점: 새로운 데이터 범위 초과 가능

[Standardization (Z-score)]
공식: x' = (x - mean) / std
범위: 평균 0, 표준편차 1
사용: 선형 모델, PCA, 정규분포 가정
- 장점: 극단값 영향 적음, 이상치에 강함
- 단점: 범위 제한 없음

[이 프로젝트에서 Min-Max 선택한 이유]
1) SVM 알고리즘 사용 → 정규화 권장
2) 데이터 범위 사전에 파악 (나이: 21-96, 소득: 0-12596)
3) 특성 해석이 필요했음 ([0,1]이 더 직관적)

만약 이후에 선형 모델로 전환한다면 표준화를 고려할 수 있음
```

---

### Q6: 로지스틱 회귀의 수식을 설명해주세요.

**답변 전략**: 수학 - 직관 - 실무 순서

```
A: 로지스틱 회귀는 선형 모델을 확률로 변환합니다.

[수학]
선형 부분: z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ

시그모이드 함수로 [0, 1] 확률로 변환:
P(y=1|x) = 1 / (1 + e^(-z))

[직관]
- z가 크면(음수에서 멀면) → P ≈ 1 (부도)
- z가 작으면(음수) → P ≈ 0 (정상)
- z = 0일 때 → P = 0.5 (결정 경계)

[손실함수]
Binary Cross-Entropy:
L = -[y×log(ŷ) + (1-y)×log(1-ŷ)]

[우리 데이터에서]
- 수렴 빠름 (큰 데이터셋)
- 확률 해석 가능 (부도율 추정)
- 특성 중요도 파악 가능 (가중치 값)
```

**팔로우업**: "왜 로지스틱을 첫 번째 모델로 했나요?"
```
→ 선택 기준:
  1) 간단함 (baseline model로 최적)
  2) 확률 출력 (신용점수와 직결)
  3) 빠른 학습 (대용량 데이터)
  4) 다른 모델과 비교 기준
  
  SVM을 바로 쓰면 성능 개선을 측정하기 어려움
```

---

### Q7: SVM의 커널(Kernel)이 무엇인가요?

**답변 전략**: 개념 - 이미지 - 코드 순서

```
A: 커널은 데이터를 고차원으로 변환하는 함수입니다.

[선형 분리 불가능한 경우]
```
2D 평면에서: 원과 원점
[o o o]  ← 선형으로 분리 불가능
[ [o] ]
[o o o]
```

[커널 트릭]
- 2D → 3D 이상으로 변환
- 고차원에서는 선형 분리 가능
- 실제 변환 계산 없이 내적만 계산

[주요 커널들]
1) Linear: K(x,y) = x·y
   → 이미 선형 분리 가능할 때
   
2) RBF (Radial Basis Function): K(x,y) = exp(-γ||x-y||²)
   → 비선형 데이터에 강력
   → gamma: 곡선 정도 조절
   
3) Polynomial: K(x,y) = (x·y + c)^d
   → 고차 관계 모델링

[코드]
from sklearn.svm import SVC
model = SVC(kernel='rbf', gamma='scale')
```

**팔로우업**: "gamma는 뭔가요?"
```
→ RBF 커널에서 각 샘플의 영향력 범위입니다.
   - gamma 크면: 가까운 점만 영향 (복잡한 경계)
   - gamma 작으면: 먼 점도 영향 (부드러운 경계)
   
우리는 'scale' 사용: gamma = 1/(n_features × X.var())
```

---

## 문제 해결 능력 질문

### Q8: 모델이 예상보다 성능이 낮으면 어떻게 하나요?

**답변 전략**: 체계적인 디버깅 프로세스

```
A: 다음과 같은 순서로 진단합니다.

[1단계: 데이터 점검]
□ 결측치 확인
□ 이상치 비정상 제거 여부
□ 클래스 분포 확인
□ 특성 스케일 확인

[2단계: 전처리 점검]
□ 정규화/표준화 적용 여부
□ 필요 없는 특성 제거
□ 특성 공학 추가

[3단계: 모델 점검]
□ 과적합 확인 (훈련 vs 테스트)
  - 훈련 정확도 90%, 테스트 70% → 과적합
  - 훈련 정확도 70%, 테스트 70% → 과소적합
  
□ 하이퍼파라미터 최적화
□ 다른 알고리즘 시도

[4단계: 메트릭 점검]
□ 올바른 메트릭 선택?
□ 클래스 불균형 고려?

[구체적 해결책]
과적합 → 정규화(L1/L2), Dropout
과소적합 → 더 복잡한 모델, 더 많은 특성
낮은 recall → 임계값 조정, 클래스 가중치
```

---

### Q9: 만약 새로운 고객 데이터가 오면?

**답변 전략**: 실무 관점에서 고려사항

```
A: 여러 문제를 고려해야 합니다.

[1] 데이터 드리프트 (Data Drift)
문제: 시간이 지나면 데이터 분포가 변함
예: 2008년 금융위기 이후 부도율 급증

해결:
□ 주기적 재훈련
□ 모니터링 대시보드
□ 성능 저하 감지 시스템

[2] 특성 드리프트 (Feature Drift)
예: 새로운 특성 추가, 기존 특성 의미 변화

[3] 스케일 차이
예: 학습 데이터 범위 [0, 12596], 새 데이터 [20000, 50000]

해결:
```python
# 훈련 시 저장한 min/max 사용
scaler = MinMaxScaler()
scaler.fit(X_train)  # 훈련 데이터의 범위로 고정

# 새 데이터에 적용
X_new_scaled = scaler.transform(X_new)
y_pred = model.predict(X_new_scaled)
```

[4] A/B 테스팅
□ 기존 모델 vs 새 모델 비교
□ 통계적 유의성 검증

[결론]
모델 배포 ≠ 끝
지속적 모니터링과 유지보수 필수
```

---

## 역량 확인 질문

### Q10: 이 프로젝트를 다시 한다면 뭘 다르게 하겠어요?

**답변 전략**: 자기성찰, 성장 마인드

```
A: 세 가지를 개선하고 싶습니다.

[1] 더 깊은 탐색적 분석 (EDA)
현재: 기본 통계, 상관관계만 분석
개선안:
□ 비타겟 특성 간 관계 심화 분석
□ 카테고리형 특성 인코딩 전략 고도화
□ 교호작용(Interaction) 항 분석

[2] 더 체계적인 모델 선택
현재: Logistic Regression, SVM만 비교
개선안:
□ Random Forest 추가
□ Gradient Boosting (XGBoost)
□ 앙상블 기법 (Voting, Stacking)
□ 신경망도 시도

[3] 자동화된 하이퍼파라미터 튜닝
현재: 수동 설정
개선안:
□ GridSearchCV로 자동 탐색
□ Bayesian Optimization
□ 결과 기록 및 비교 자동화

[4] 배포까지 고려
현재: 모델 훈련까지만
개선안:
□ Flask/FastAPI REST API
□ Docker 컨테이너화
□ 모니터링 시스템
```

---

### Q11: 팀 프로젝트 경험이 있나요?

**답변 전략**: 없으면 인정하고, 있으면 배운 점 강조

```
A: 이 프로젝트는 개인 프로젝트였지만, 협업을 고려했습니다.

[협업을 위한 시도]
□ Git으로 버전 관리
□ 명확한 폴더 구조 (src/, docs/, data/)
□ 모듈화된 코드 작성
□ 상세한 주석과 문서화

[만약 팀이었다면]
1) 코드 리뷰 프로세스
   - PR(Pull Request) 기반
   - 최소 2명 승인 필수
   
2) 문서 공유
   - README 정독 필수
   - 설정 가이드 제공
   
3) 작업 분담
   - 한 명: EDA 담당
   - 한 명: 전처리 담당
   - 한 명: 모델링 담당
   
4) 정기 회의
   - 주 1회 진행상황 공유
   - 병목 상황 해결
```

---

### Q12: 이 기술을 실무에 어떻게 적용할 수 있을까요?

**답변 전략**: 현실적 응용, 가치 제시

```
A: 금융/신용카드 회사, P2P 대출 플랫폼 등에서 활용 가능합니다.

[1] 신용등급 결정
현재: 정성적 판단
제안: 모델 기반 자동화
- 빠른 심사 (수 초 내)
- 일관된 기준
- 편향 감소

[2] 리스크 관리
- 부도 위험이 높은 고객 사전 탐지
- 적절한 이자율 책정
- 담보 설정 기준 수립

[3] 마케팅 타겟팅
- 부도 위험 낮은 고객 → 공격적 마케팅
- 부도 위험 높은 고객 → 신중한 대출

[4] 규제 준수
- 모델 기반 객관적 판단 기록
- 감시 기관 보고
- 비차별성 입증

[기술적 구현]
□ 특성 엔지니어링: 실시간 신용 정보 활용
□ 모니터링: 매월 모델 성능 점검
□ 재훈련: 새로운 부도 패턴 학습
□ A/B 테스팅: 모델 개선 검증

[예상 효과]
- 부도율 감소: 5~10%
- 심사 속도 향상: 10배
- 운영 비용 절감: 자동화
```

---

## 📝 보너스: 어려운 질문 대비

### Q13: "Recall과 Precision 사이의 트레이드오프가 뭔가요?"

```
A: 임계값 조정으로 두 메트릭 간 균형을 조절합니다.

[기본: 임계값 = 0.5]
P(부도) > 0.5면 부도로 예측

[임계값 낮춤: 0.3]
- Recall ↑ (더 많은 부도자 탐지)
- Precision ↓ (오경보 증가)
- 은행 입장: 안전하지만 기회 손실

[임계값 올림: 0.7]
- Recall ↓ (부도자 일부 놓침)
- Precision ↑ (정확한 판단)
- 은행 입장: 위험하지만 효율적

[최적값 찾기]
업무: ROC 곡선이나 Precision-Recall 곡선으로 최적값 선택
우리 데이터: F1-Score 기준으로 임계값 0.4~0.5 추천
```

---

### Q14: "왜 테스트 데이터로는 하이퍼파라미터를 조정하면 안 되나요?"

```
A: 데이터 누수(Data Leakage) 문제 때문입니다.

[잘못된 접근]
테스트 데이터로 평가 → 성능 나쁨 → 파라미터 수정 → 재평가
→ 테스트 데이터에 오버피팅됨
→ 실제 성능: 매우 낮을 가능성

[올바른 접근]
훈련 데이터: 모델 학습 (80%)
검증 데이터: 파라미터 조정 (10%)  ← 여기서만 조정
테스트 데이터: 최종 평가 (10%)     ← 한 번만 사용

또는 교차 검증:
각 폴드마다 다른 검증 데이터 사용
```

---

## 🎯 면접 최종 체크리스트

- [ ] 프로젝트 목표 명확히 설명 가능?
- [ ] 데이터 전처리 과정 설명 가능?
- [ ] 각 알고리즘 선택 이유 설명 가능?
- [ ] 평가 메트릭 선택 이유 설명 가능?
- [ ] 겪은 문제와 해결 방법 구체적으로?
- [ ] 배운 점 명확히?
- [ ] 실무 적용 아이디어 있는가?
- [ ] 앞으로의 개선 방안 생각해봤는가?

---

## 💡 면접 대답 팁

1. **구체적으로**: "좋았어요" X → "Recall이 85%까지 개선됐어요" O
2. **왜를 설명**: "SVM을 썼어요" X → "정규화된 데이터에서 경계가 비선형이므로 RBF 커널의 SVM을 선택했어요" O
3. **문제를 인정**: "완벽했어요" X → "처음엔 Accuracy만 봤는데 나중에 F1-Score의 중요성을 깨달았어요" O
4. **배운 점 강조**: 단순 설명 X → "이를 통해 비즈니스 문맥이 얼마나 중요한지 배웠습니다" O

---

**면접 화이팅! 🚀**
